#!/usr/bin/env python3
"""
–î–µ–º–æ-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –µ–≥–æ
"""

import os
from summarizer import TextSummarizer

def demo_text_analysis():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –ò–ò"""
    
    print("üéØ –î–ï–ú–û: –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –ö –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò")
    print("="*60)
    
    # –ß–∏—Ç–∞–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã
    try:
        with open('subtitles_DpQtKCcTMnQ_ru.txt', 'r', encoding='utf-8') as f:
            text = f.read()
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç —Å—É–±—Ç–∏—Ç—Ä–æ–≤")
        print(f"   –†–∞–∑–º–µ—Ä: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°–ª–æ–≤: {len(text.split())} —Å–ª–æ–≤")
        print(f"   –°—Ç—Ä–æ–∫: {text.count(chr(10)) + 1}")
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
    print(f"\nüìñ –ê–ù–ê–õ–ò–ó –°–û–î–ï–†–ñ–ê–ù–ò–Ø:")
    lines = text.split('\n')[:20]  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
    print("–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å—É–±—Ç–∏—Ç—Ä–æ–≤:")
    for i, line in enumerate(lines, 1):
        if line.strip():
            print(f"   {i:2}: {line.strip()}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä
    os.environ['OPENROUTER_API_KEY'] = 'demo'
    summarizer = TextSummarizer()
    
    print(f"\nü§ñ –î–û–°–¢–£–ü–ù–´–ï –ò–ò-–ú–û–î–ï–õ–ò:")
    for i, (name, model_id) in enumerate(summarizer.models):
        print(f"   {i+1}. {name}")
        print(f"      ID: {model_id}")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º —Ä–∞–∑–±–∏–≤–∫—É —Ç–µ–∫—Å—Ç–∞
    print(f"\n‚úÇÔ∏è  –†–ê–ó–ë–ò–í–ö–ê –¢–ï–ö–°–¢–ê –ù–ê –ß–ê–°–¢–ò:")
    chunks = summarizer.split_text(text, max_len=1000)
    print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –†–∞–∑–±–∏—Ç–æ –Ω–∞: {len(chunks)} —á–∞—Å—Ç–µ–π")
    print(f"   –†–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏: –¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤")
    
    for i, chunk in enumerate(chunks[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —á–∞—Å—Ç–∏
        print(f"\n   üìÑ –ß–∞—Å—Ç—å {i}: {len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤")
        preview = chunk[:150] + "..." if len(chunk) > 150 else chunk
        print(f"      –ü—Ä–µ–≤—å—é: {preview}")
    
    if len(chunks) > 3:
        print(f"   ... –∏ –µ—â–µ {len(chunks) - 3} —á–∞—Å—Ç–µ–π")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –°–õ–û–í:")
    words = text.lower().split()
    word_freq = {}
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
    stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–Ω–æ', '–∏–ª–∏', 
                  '—Ç–æ', '—Ç–∞–∫', '–≤—Å–µ', '—É–∂–µ', '–µ—â—ë', '–±—É–¥–µ—Ç', '–µ—Å–ª–∏', '–º–æ–∂–µ—Ç', '–æ—á–µ–Ω—å'}
    
    for word in words:
        clean_word = ''.join(c for c in word if c.isalpha())
        if len(clean_word) > 3 and clean_word not in stop_words:
            word_freq[clean_word] = word_freq.get(clean_word, 0) + 1
    
    # –¢–æ–ø-10 —Å–ª–æ–≤
    top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
    print("   –¢–æ–ø-10 —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤:")
    for word, count in top_words:
        print(f"      {word}: {count} —Ä–∞–∑")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º–∞—Ç–∏–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    print(f"\nüéØ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ï–ú–ê–¢–ò–ö–ò:")
    astro_words = ['—Å–∞—Ç—É—Ä–Ω', '—Ä—ã–±–∞—Ö', '—Ç—Ä–∞–Ω–∑–∏—Ç', '–∑–Ω–∞–∫', '–ª–∞–≥–Ω–∞', '—Å–∞–¥—ã', '–ø–ª–∞–Ω–µ—Ç–∞']
    found_astro = [word for word, _ in top_words if any(aw in word for aw in astro_words)]
    
    if found_astro:
        print("   ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞")
        print(f"   –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {', '.join(found_astro)}")
    else:
        print("   üîç –¢–µ–º–∞—Ç–∏–∫–∞ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    
    print(f"\nüìã –°–¢–†–£–ö–¢–£–†–ê –î–õ–Ø –ò–ò-–°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò:")
    print("   –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∞ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ò–ò:")
    print(f"   ‚úÖ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —á–∞—Å—Ç–µ–π")
    print("   ‚úÖ –í—ã—è–≤–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Ç–µ–º–∞—Ç–∏–∫–∞")
    print("   ‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω –æ–±—ä–µ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    print("   ü§ñ –ì–æ—Ç–æ–≤–æ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –ò–ò-–º–æ–¥–µ–ª—è–º")

def demo_prompts():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –ò–ò"""
    
    print(f"\nüé≠ –ü–†–û–ú–ü–¢–´ –î–õ–Ø –ò–ò-–ê–ù–ê–õ–ò–ó–ê:")
    print("="*40)
    
    prompts = [
        {
            "name": "–ë–∞–∑–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è",
            "prompt": "–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –í—ã–¥–µ–ª–∏ –≥–ª–∞–≤–Ω—ã–µ –º—ã—Å–ª–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã."
        },
        {
            "name": "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            "prompt": """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –∏ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é:

1. –ì–õ–ê–í–ù–ê–Ø –¢–ï–ú–ê (–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)
2. –ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´ (3-5 –ø—É–Ω–∫—Ç–æ–≤)  
3. –û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
4. –û –ß–ï–ú –≠–¢–û–¢ –¢–ï–ö–°–¢ (–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ)

–ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º."""
        },
        {
            "name": "–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            "prompt": """–í—ã–ø–æ–ª–Ω–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è:

‚Ä¢ –û —á–µ–º –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ?
‚Ä¢ –ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã?
‚Ä¢ –ö–∞–∫–∏–µ –≤—ã–≤–æ–¥—ã –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å?
‚Ä¢ –ö–∞–∫–∞—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–ª—å–∑–∞ –æ—Ç —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏?

–û—Ç–≤–µ—Ç—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ."""
        }
    ]
    
    for i, prompt_data in enumerate(prompts, 1):
        print(f"\nü§ñ –ü–†–û–ú–ü–¢ {i}: {prompt_data['name']}")
        print("-" * 30)
        print(prompt_data['prompt'])
        print("-" * 30)

def show_testing_commands():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–º–∞–Ω–¥—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    print(f"\nüöÄ –ö–û–ú–ê–ù–î–´ –î–õ–Ø –†–ï–ê–õ–¨–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    print("="*50)
    
    print("1Ô∏è‚É£  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º API:")
    print("   export OPENROUTER_API_KEY='your-key'")
    print("   python test_summarization_quality.py")
    
    print("\n2Ô∏è‚É£  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –±–æ—Ç–∞:")
    print("   python bot.py")
    print("   # –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É: https://youtu.be/DpQtKCcTMnQ")
    
    print("\n3Ô∏è‚É£  –¢–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º –≤–∏–¥–µ–æ:")
    print("   python tests/test_real_video.py")
    
    print("\n4Ô∏è‚É£  –ü—Ä—è–º–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞:")
    print("""   python -c "
import asyncio
from summarizer import TextSummarizer
s = TextSummarizer()
text = open('subtitles_DpQtKCcTMnQ_ru.txt').read()[:2000]
result = asyncio.run(s.summarize_text(text, 0))
print(result[0])
   \"""")

if __name__ == "__main__":
    print("üé¨ –î–ï–ú–û-–ê–ù–ê–õ–ò–ó YOUTUBE –°–£–ë–¢–ò–¢–†–û–í")
    print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ò–ò-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
    print()
    
    demo_text_analysis()
    demo_prompts()
    show_testing_commands()
    
    print(f"\nüéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É–±—Ç–∏—Ç—Ä—ã")
    print("‚úÖ –¢–µ–∫—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –ò–ò")
    print("‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ç–µ–º–∞—Ç–∏–∫–∞ (–∞—Å—Ç—Ä–æ–ª–æ–≥–∏—è)")
    print("‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã")
    print("ü§ñ –ì–æ—Ç–æ–≤–æ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ò–ò-–º–æ–¥–µ–ª—è–º–∏!") 