import os
import requests
import re
import time
import asyncio
import random
import logging
from typing import List, Optional, Tuple, Dict
from dotenv import load_dotenv

load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
summarization_logger = logging.getLogger('summarization')
summarization_logger.setLevel(logging.INFO)

# –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ª–æ–≥–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
if not summarization_logger.handlers:
    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –ø–∞–ø–∫–∞ logs —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs('logs', exist_ok=True)
    summarization_handler = logging.FileHandler('logs/summarization.log', encoding='utf-8-sig')
    summarization_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    summarization_handler.setFormatter(summarization_formatter)
    summarization_logger.addHandler(summarization_handler)

class TextSummarizer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenRouter API —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–∏"""
    
    def __init__(self):
        self.api_key = os.getenv('OPENROUTER_API_KEY')
        self.chunk_size = 1000
        self.max_retries = 3
        self.retry_delay = 5
        self.rate_limit_delay = 60
        
        # –õ–∏–º–∏—Ç—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        self.warning_threshold = 15000  # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ > 15K —Å–∏–º–≤–æ–ª–æ–≤
        self.max_text_length = 50000    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.large_text_threshold = 30000  # –ü–æ—Ä–æ–≥ –¥–ª—è "–±–æ–ª—å—à–∏—Ö" —Ç–µ–∫—Å—Ç–æ–≤
        
        # –°–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter
        self.models = [
            ("venice_uncensored", "venice/uncensored:free"),
            ("google_gemma_3n_2b", "google/gemma-3n-e2b-it:free"),
            ("tencent_hunyuan_a13b", "tencent/hunyuan-a13b-instruct:free"),
            ("tng_deepseek_r1t2_chimera", "tngtech/deepseek-r1t2-chimera:free"),
            ("cypher_alpha", "openrouter/cypher-alpha:free"),
            ("mistral_small_3_2_24b", "mistralai/mistral-small-3.2-24b-instruct:free"),
            ("kimi_dev_72b", "moonshotai/kimi-dev-72b:free"),
            ("deepseek_r1_0528", "deepseek/deepseek-r1-0528:free"),
        ]
        
        # –ò—Å—Ç–æ—Ä–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        self.used_models = set()
        
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://openrouter.ai/",
            "X-Title": "Telegram Subs-bot"
        }
    
    def check_text_length(self, text: str) -> dict:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            dict: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        text_length = len(text)
        
        result = {
            "length": text_length,
            "chunks": len(self.split_text(text)),
            "status": "ok",
            "warning": None,
            "can_process": True
        }
        
        if text_length > self.max_text_length:
            result["status"] = "too_long"
            result["warning"] = f"‚ùå –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π ({text_length:,} —Å–∏–º–≤–æ–ª–æ–≤). –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {self.max_text_length:,} —Å–∏–º–≤–æ–ª–æ–≤."
            result["can_process"] = False
        elif text_length > self.large_text_threshold:
            result["status"] = "very_large"
            result["warning"] = f"‚ö†Ô∏è –¢–µ–∫—Å—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π ({text_length:,} —Å–∏–º–≤–æ–ª–æ–≤, {result['chunks']} —á–∞—Å—Ç–µ–π). –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-15 –º–∏–Ω—É—Ç."
        elif text_length > self.warning_threshold:
            result["status"] = "large"
            result["warning"] = f"üìù –¢–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω—ã–π ({text_length:,} —Å–∏–º–≤–æ–ª–æ–≤, {result['chunks']} —á–∞—Å—Ç–µ–π). –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–π–º–µ—Ç 5-10 –º–∏–Ω—É—Ç."
        
        return result
    
    def get_available_model_index(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏, –∏–∑–±–µ–≥–∞—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö"""
        available_models = [i for i in range(len(self.models)) if i not in self.used_models]
        
        if not available_models:
            # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã, —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            self.used_models.clear()
            available_models = list(range(len(self.models)))
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
        selected_index = random.choice(available_models)
        self.used_models.add(selected_index)
        
        summarization_logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {self.models[selected_index][0]} (–∏–Ω–¥–µ–∫—Å: {selected_index})")
        return selected_index
    
    def log_summarization_result(self, success: bool, model_name: str, text_length: int, 
                                summary_length: int, chunks: int, error: str = None, 
                                user_satisfied: bool = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if success:
            satisfaction_info = f", –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ–≤–æ–ª–µ–Ω: {user_satisfied}" if user_satisfied is not None else ""
            summarization_logger.info(
                f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –£–°–ü–ï–®–ù–ê - –ú–æ–¥–µ–ª—å: {model_name}, "
                f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text_length} —Å–∏–º–≤–æ–ª–æ–≤, "
                f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {summary_length} —Å–∏–º–≤–æ–ª–æ–≤, "
                f"–ß–∞—Å—Ç–µ–π: {chunks}{satisfaction_info}"
            )
        else:
            summarization_logger.error(
                f"‚ùå –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ù–ï–£–°–ü–ï–®–ù–ê - –ú–æ–¥–µ–ª—å: {model_name}, "
                f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text_length} —Å–∏–º–≤–æ–ª–æ–≤, "
                f"–û—à–∏–±–∫–∞: {error}, "
                f"–ß–∞—Å—Ç–µ–π: {chunks}"
            )
    
    def log_user_feedback(self, model_name: str, text_length: int, summary_length: int, 
                          user_satisfied: bool, feedback_reason: str = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        feedback_text = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {'–¥–æ–≤–æ–ª–µ–Ω' if user_satisfied else '–Ω–µ –¥–æ–≤–æ–ª–µ–Ω'}"
        if feedback_reason:
            feedback_text += f" - –ü—Ä–∏—á–∏–Ω–∞: {feedback_reason}"
        
        summarization_logger.info(
            f"üë§ –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å - –ú–æ–¥–µ–ª—å: {model_name}, "
            f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text_length} —Å–∏–º–≤–æ–ª–æ–≤, "
            f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {summary_length} —Å–∏–º–≤–æ–ª–æ–≤, "
            f"{feedback_text}"
        )
    
    def detect_language(self, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫
        Returns: –∫–æ–¥ —è–∑—ã–∫–∞ ('ru', 'en', 'other')
        """
        # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞
        text_lower = text.lower()
        
        # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
        russian_chars = re.findall(r'[–∞-—è—ë]', text_lower)
        russian_ratio = len(russian_chars) / len(text_lower.replace(' ', '')) if text_lower.replace(' ', '') else 0
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫
        english_chars = re.findall(r'[a-z]', text_lower)
        english_ratio = len(english_chars) / len(text_lower.replace(' ', '')) if text_lower.replace(' ', '') else 0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –ø–æ –ø—Ä–µ–æ–±–ª–∞–¥–∞–Ω–∏—é —Å–∏–º–≤–æ–ª–æ–≤
        if russian_ratio > 0.3:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 30% —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            return 'ru'
        elif english_ratio > 0.5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 50% –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            return 'en'
        else:
            return 'other'
    
    async def translate_to_russian(self, text: str, source_lang: str = None) -> str:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–∏
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
            source_lang: –∏—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω)
        Returns:
            –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        if not self.api_key:
            return text
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
        model_index = self.get_available_model_index()
        model_name, model_id = self.models[model_index]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
        if source_lang == 'en':
            prompt = "–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å–º—ã—Å–ª –∏ —Å—Ç–∏–ª—å:"
        elif source_lang == 'other':
            prompt = "–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å–º—ã—Å–ª –∏ —Å—Ç–∏–ª—å:"
        else:
            prompt = "–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, —Å–æ—Ö—Ä–∞–Ω–∏–≤ —Å–º—ã—Å–ª –∏ —Å—Ç–∏–ª—å:"
        
        messages = [
            {"role": "system", "content": """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫. 

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–µ—Ä–µ–≤–æ–¥—É:**
‚Ä¢ –ü–µ—Ä–µ–≤–æ–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–π —Å–º—ã—Å–ª, —Å—Ç–∏–ª—å –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∞–±–∑–∞—Ü—ã, **–∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç**, —Å–ø–∏—Å–∫–∏)
‚Ä¢ –ê–¥–∞–ø—Ç–∏—Ä—É–π –ø–æ–¥ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ä—É—Å—Å–∫—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é"""},
            {"role": "user", "content": f"{prompt}\n\n{text}"}
        ]
        
        try:
            result = await self._make_request(model_id, messages)
            if result and not result.startswith("‚ùå"):
                # –£—Å–ø–µ—à–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
                summarization_logger.info(
                    f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –£–°–ü–ï–®–ï–ù - –ú–æ–¥–µ–ª—å: {model_name}, "
                    f"–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: {source_lang or 'unknown'}, "
                    f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, "
                    f"–ü–µ—Ä–µ–≤–æ–¥: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤"
                )
                return result
            else:
                # –ù–µ—É—Å–ø–µ—à–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
                summarization_logger.error(
                    f"‚ùå –ü–µ—Ä–µ–≤–æ–¥ –ù–ï–£–°–ü–ï–®–ï–ù - –ú–æ–¥–µ–ª—å: {model_name}, "
                    f"–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: {source_lang or 'unknown'}, "
                    f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, "
                    f"–û—à–∏–±–∫–∞: {result or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}"
                )
                return text
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}"
            summarization_logger.error(
                f"‚ùå –ü–µ—Ä–µ–≤–æ–¥ –ù–ï–£–°–ü–ï–®–ï–ù - –ú–æ–¥–µ–ª—å: {model_name}, "
                f"–ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫: {source_lang or 'unknown'}, "
                f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, "
                f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {error_msg}"
            )
            print(error_msg)
            return text
    
    def split_text(self, text: str, max_len: int = None) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—è —Å–ª–æ–≤–∞"""
        max_len = max_len or self.chunk_size
        words = re.findall(r'\S+|\n', text)
        chunks = []
        current = ''
        
        for word in words:
            if len(current) + len(word) + 1 > max_len:
                if current.strip():
                    chunks.append(current.strip())
                current = word if word != '\n' else ''
            else:
                current += (' ' if current and word != '\n' and not current.endswith('\n') else '') + word
        
        if current.strip():
            chunks.append(current.strip())
        
        return chunks
    
    async def _make_request(self, model_id: str, messages: List[dict]) -> Optional[str]:
        """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ OpenRouter API —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        data = {
            "model": model_id,
            "messages": messages
        }
        
        for attempt in range(1, self.max_retries + 1):
            try:
                # –î–µ–ª–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: requests.post(
                        "https://openrouter.ai/api/v1/chat/completions",
                        headers=self.headers,
                        json=data,
                        timeout=120
                    )
                )
                
                response.raise_for_status()
                result = response.json()
                
                # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                content = result["choices"][0]["message"]["content"]
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                usage = result.get("usage", {})
                model_info = result.get("model", model_id)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                if not hasattr(self, '_last_api_info'):
                    self._last_api_info = {}
                
                self._last_api_info = {
                    "model_used": model_info,
                    "prompt_tokens": usage.get("prompt_tokens", 0),
                    "completion_tokens": usage.get("completion_tokens", 0),
                    "total_tokens": usage.get("total_tokens", 0),
                    "finish_reason": result["choices"][0].get("finish_reason", "unknown")
                }
                
                return content
                
            except Exception as e:
                if hasattr(e, 'response') and getattr(e.response, 'status_code', None) == 429:
                    # Rate limit - –∂–¥–µ–º –¥–æ–ª—å—à–µ
                    await asyncio.sleep(self.rate_limit_delay)
                elif attempt < self.max_retries:
                    await asyncio.sleep(self.retry_delay)
                else:
                    return f"–û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {self.max_retries} –ø–æ–ø—ã—Ç–æ–∫: {str(e)}"
        
        return None
    
    async def summarize_text(self, text: str, custom_prompt: str = None, forced_model_index: int = None) -> Tuple[str, dict]:
        """
        –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ —á–∞—Å—Ç—è–º —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–∏
        
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            custom_prompt: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            forced_model_index: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –º–æ–¥–µ–ª–∏ (–¥–ª—è retry –º–µ—Ö–∞–Ω–∏–∑–º–∞)
        
        Returns:
            Tuple[str, dict]: (–∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        """
        if not self.api_key:
            error_msg = "‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω OPENROUTER_API_KEY"
            self.log_summarization_result(False, "none", len(text), 0, 0, error_msg)
            return error_msg, {}
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å: –ª–∏–±–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ, –ª–∏–±–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        if forced_model_index is not None:
            model_index = forced_model_index
        else:
            model_index = self.get_available_model_index()
        
        model_name, model_id = self.models[model_index]
        
        prompt = custom_prompt or (
            "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫—Ä–∞—Ç–∫–∏—Ö –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π.\n"
            "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ–æ—Ä–º–∞—Ç—É (Markdown):\n"
            "‚Ä¢ –†–∞–∑–±–∏–≤–∞–π –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Ä–∞–∑–¥–µ–ª—è–π –∞–±–∑–∞—Ü—ã –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.\n"
            "‚Ä¢ –í—ã–¥–µ–ª—è–π –∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è **–∂–∏—Ä–Ω—ã–º**.\n"
            "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Ç–∞–º, –≥–¥–µ —É–º–µ—Å—Ç–Ω–æ.\n"
            "‚Ä¢ –î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö/–º–∞—Ä–∫–µ—Ä–∞—Ö (–Ω–µ –∑–ª–æ—É–ø–æ—Ç—Ä–µ–±–ª—è–π).\n"
            "‚Ä¢ –ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏, —Å–æ–±–ª—é–¥–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é."
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        source_language = self.detect_language(text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
        chunks = self.split_text(text)
        
        if not chunks:
            error_msg = "‚ùå –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"
            self.log_summarization_result(False, model_name, len(text), 0, 0, error_msg)
            return error_msg, {}
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ä–∞–∑—É
        if len(chunks) == 1 and len(text) < 2000:
            messages = [
                {
                    "role": "system",
                    "content": (
                        "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –∫—Ä–∞—Ç–∫–∏—Ö –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π. "
                        "–°—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π Markdown-–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ: –∞–±–∑–∞—Ü—ã —Å –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏, **–∂–∏—Ä–Ω—ã–µ** –∫–ª—é—á–∏, —Å–ø–∏—Å–∫–∏; "
                        "—É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã."
                    )
                },
                {"role": "user", "content": text}
            ]
            result = await self._make_request(model_id, messages)
            
            if result and not result.startswith("‚ùå"):
                # –£—Å–ø–µ—à–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
                stats = {
                    "model": model_name,
                    "chunks": 1,
                    "original_length": len(text),
                    "summary_length": len(result),
                    "source_language": source_language
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç API
                if hasattr(self, '_last_api_info'):
                    stats.update(self._last_api_info)
                self.log_summarization_result(True, model_name, len(text), len(result), 1)
                return result, stats
            else:
                # –ù–µ—É—Å–ø–µ—à–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
                error_msg = result or "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é"
                stats = {
                    "model": model_name,
                    "chunks": 1,
                    "original_length": len(text),
                    "summary_length": 0,
                    "source_language": source_language
                }
                self.log_summarization_result(False, model_name, len(text), 0, 1, error_msg)
                return error_msg, stats
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º
        chunk_summaries = []
        successful_chunks = 0
        
        for i, chunk in enumerate(chunks):
            messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": chunk}
            ]
            
            summary = await self._make_request(model_id, messages)
            
            if summary and not summary.startswith("‚ùå"):
                chunk_summaries.append(summary)
                successful_chunks += 1
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(chunks) - 1:
                await asyncio.sleep(2)
        
        if not chunk_summaries:
            error_msg = "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –Ω–∏ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏"
            self.log_summarization_result(False, model_name, len(text), 0, len(chunks), error_msg)
            return error_msg, {}
        
        # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        final_prompt = (
            "–°–æ–∑–¥–∞–π –µ–¥–∏–Ω—É—é —Å–≤—è–∑–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. "
            "–°—Ç—Ä–æ–≥–æ –æ—Ñ–æ—Ä–º–∏ –≤ Markdown: –∞–±–∑–∞—Ü—ã —Ä–∞–∑–¥–µ–ª—è–π –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π, –≤—ã–¥–µ–ª—è–π **–∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è**, "
            "–∏—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Ç–∞–º, –≥–¥–µ —ç—Ç–æ —É–ª—É—á—à–∞–µ—Ç —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å, –¥–æ–ø—É—Å–∫–∞–π —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏."
        )
        combined_text = "\n\n".join(chunk_summaries)
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        final_summary = await self._create_final_summary_with_retry(model_id, combined_text, final_prompt)
        
        if final_summary and not final_summary.startswith("‚ùå"):
            # –£—Å–ø–µ—à–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            stats = {
                "model": model_name,
                "chunks": len(chunks),
                "original_length": len(text),
                "summary_length": len(final_summary),
                "processed_chunks": successful_chunks,
                "source_language": source_language
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç API
            if hasattr(self, '_last_api_info'):
                stats.update(self._last_api_info)
            self.log_summarization_result(True, model_name, len(text), len(final_summary), len(chunks))
            return final_summary, stats
        else:
            # –ù–µ—É—Å–ø–µ—à–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            error_msg = final_summary or "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏—Ç–æ–≥–æ–≤—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é"
            
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if chunk_summaries:
                fallback_summary = self._create_fallback_summary(chunk_summaries)
                warning_msg = f"‚ö†Ô∏è –ò—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å. –í–æ–∑–≤—Ä–∞—â–∞—é –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n\n{fallback_summary}"
                
                stats = {
                    "model": model_name,
                    "chunks": len(chunks),
                    "original_length": len(text),
                    "summary_length": len(fallback_summary),
                    "processed_chunks": successful_chunks,
                    "source_language": source_language,
                    "fallback_used": True,
                    "final_summary_failed": True
                }
                
                if hasattr(self, '_last_api_info'):
                    stats.update(self._last_api_info)
                
                self.log_summarization_result(False, model_name, len(text), len(fallback_summary), len(chunks), error_msg)
                return warning_msg, stats
            else:
                # –ü–æ–ª–Ω—ã–π –ø—Ä–æ–≤–∞–ª
                stats = {
                    "model": model_name,
                    "chunks": len(chunks),
                    "original_length": len(text),
                    "summary_length": 0,
                    "processed_chunks": successful_chunks,
                    "source_language": source_language
                }
                self.log_summarization_result(False, model_name, len(text), 0, len(chunks), error_msg)
                return error_msg, stats
    
    async def summarize_with_retry(self, text: str, custom_prompt: str = None, max_retries: int = 3) -> Tuple[str, dict, bool]:
        """
        –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            custom_prompt: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            max_retries: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        
        Returns:
            Tuple[str, dict, bool]: (—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å)
        """
        used_models = []
        last_error = None
        
        for attempt in range(max_retries):
            # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å, –∏–∑–±–µ–≥–∞—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö
            model_index = self.get_available_model_index()
            model_name, model_id = self.models[model_index]
            used_models.append(model_name)
            
            summarization_logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} —Å –º–æ–¥–µ–ª—å—é: {model_name}")
            
            try:
                summary, stats = await self.summarize_text(text, custom_prompt, model_index)
                success = not summary.startswith("‚ùå")
                
                if success:
                    summarization_logger.info(f"‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞ —Å –º–æ–¥–µ–ª—å—é {model_name} –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}")
                    return summary, stats, True
                else:
                    last_error = summary
                    summarization_logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} —Å –º–æ–¥–µ–ª—å—é {model_name} –Ω–µ —É–¥–∞–ª–∞—Å—å: {summary}")
                    
            except Exception as e:
                last_error = f"–û—à–∏–±–∫–∞: {str(e)}"
                summarization_logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} —Å –º–æ–¥–µ–ª—å—é {model_name}: {e}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if attempt < max_retries - 1:
                await asyncio.sleep(2)
        
        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        error_msg = f"‚ùå –í—Å–µ {max_retries} –ø–æ–ø—ã—Ç–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(used_models)}. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}"
        summarization_logger.error(error_msg)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ
        failed_stats = {
            "model": "multiple_attempts",
            "chunks": 0,
            "original_length": len(text),
            "summary_length": 0,
            "used_models": used_models,
            "last_error": last_error
        }
        
        return error_msg, failed_stats, False
    
    async def _create_final_summary_with_retry(self, model_id: str, combined_text: str, prompt: str, max_retries: int = 3) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            combined_text: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
            prompt: –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            max_retries: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            
        Returns:
            –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
        """
        for attempt in range(1, max_retries + 1):
            try:
                messages = [
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": combined_text}
                ]
                
                result = await self._make_request(model_id, messages)
                
                if result and not result.startswith("‚ùå"):
                    return result
                else:
                    summarization_logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries} –∏—Ç–æ–≥–æ–≤–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å: {result}")
                    
            except Exception as e:
                summarization_logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ {attempt}/{max_retries} –∏—Ç–æ–≥–æ–≤–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if attempt < max_retries:
                await asyncio.sleep(self.retry_delay * attempt)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É —Å –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
        
        return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏—Ç–æ–≥–æ–≤—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫"
    
    def _create_fallback_summary(self, chunk_summaries: List[str]) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç fallback —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –∏–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            chunk_summaries: —Å–ø–∏—Å–æ–∫ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–π —á–∞—Å—Ç–µ–π
            
        Returns:
            fallback —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        """
        if not chunk_summaries:
            return "‚ùå –ù–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è fallback —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        fallback_parts = []
        for i, summary in enumerate(chunk_summaries, 1):
            fallback_parts.append(f"**–ß–∞—Å—Ç—å {i}:**\n{summary}")
        
        fallback_text = "\n\n---\n\n".join(fallback_parts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        header = f"‚ö†Ô∏è **–ü–†–û–ú–ï–ñ–£–¢–û–ß–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´**\n\n*–ò—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –Ω–æ –≤–æ—Ç —á—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ —á–∞—Å—Ç—è–º:*\n\n"
        
        return header + fallback_text
    
    async def summarize_with_feedback(self, text: str, custom_prompt: str = None) -> Tuple[str, dict, bool]:
        """
        –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        
        Args:
            text: —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            custom_prompt: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            Tuple[str, dict, bool]: (—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å)
        """
        summary, stats = await self.summarize_text(text, custom_prompt)
        success = not summary.startswith("‚ùå")
        return summary, stats, success
    
    def get_available_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return [name for name, _ in self.models]
    
    def get_model_usage_stats(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏"""
        return {self.models[i][0]: 1 for i in self.used_models} 